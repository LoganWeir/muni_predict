{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.106:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.106:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1054caeb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo 149\n"
     ]
    }
   ],
   "source": [
    "cln_list = []\n",
    "\n",
    "with open('data/avl/sfmtaAVLRawData01032016.csv') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) > 125:\n",
    "            cln_list.append(line[0:89])\n",
    "            cln_list.append(line[89:])\n",
    "        \n",
    "        if len(line) != 0 and len(line) < 125:\n",
    "            cln_list.append(line)\n",
    "\n",
    "with open('data/avl/clean_sample.csv', 'w') as outfile:\n",
    "    for row in cln_list:\n",
    "        outfile.write(row + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv('data/avl/clean_sample.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- REV: integer (nullable = true)\n",
      " |-- REPORT_TIME: string (nullable = true)\n",
      " |-- VEHICLE_TAG: string (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- SPEED: double (nullable = true)\n",
      " |-- HEADING: double (nullable = true)\n",
      " |-- TRAIN_ASSIGNMENT: string (nullable = true)\n",
      " |-- PREDICTABLE: integer (nullable = true)\n",
      "\n",
      "line count: 494778\n",
      "+----+-------------------+-----------+----------+--------+-----+-------+----------------+-----------+\n",
      "| REV|        REPORT_TIME|VEHICLE_TAG| LONGITUDE|LATITUDE|SPEED|HEADING|TRAIN_ASSIGNMENT|PREDICTABLE|\n",
      "+----+-------------------+-----------+----------+--------+-----+-------+----------------+-----------+\n",
      "|1506|01/03/2016 00:03:29|         01|-122.42047|37.80633|  0.0|  201.0|            null|          0|\n",
      "|1506|01/03/2016 00:04:59|         01|-122.42049|37.80638|  0.0|  256.0|            null|          0|\n",
      "|1506|01/03/2016 00:12:29|         01|-122.42092| 37.8069|  0.0|  247.0|            null|          0|\n",
      "|1506|01/03/2016 00:13:59|         01|-122.42091| 37.8069|  0.0|    0.0|            null|          0|\n",
      "|1506|01/03/2016 00:21:29|         01| -122.4209|37.80691|  0.0|  259.0|            null|          0|\n",
      "|1506|01/03/2016 00:22:59|         01|-122.42121|37.80705|  0.0|  281.0|            null|          0|\n",
      "|1506|01/03/2016 00:25:59|         01|-122.42099|37.80693|  0.0|    0.0|            null|          0|\n",
      "|1506|01/03/2016 00:26:35|         01|-122.42046|37.80625|3.889|  170.0|            null|          0|\n",
      "|1506|01/03/2016 00:26:53|         01|-122.42032|37.80556|3.333|  170.0|            null|          0|\n",
      "|1506|01/03/2016 00:27:48|         01|-122.41992|37.80351|3.889|  170.0|            null|          0|\n",
      "|1506|01/03/2016 00:28:06|         01|-122.41979|37.80283|3.333|  167.0|            null|          0|\n",
      "|1506|01/03/2016 00:29:10|         01|-122.41927|37.80036|6.111|  171.0|            null|          0|\n",
      "|1506|01/03/2016 00:29:52|         01|-122.41885|37.79828|3.889|  170.0|            null|          0|\n",
      "|1506|01/03/2016 00:30:17|         01|-122.41859|37.79693|6.667|  169.0|            null|          0|\n",
      "|1506|01/03/2016 00:31:16|         01|-122.41811|37.79466|3.889|  167.0|            null|          0|\n",
      "|1506|01/03/2016 00:31:35|         01|-122.41799|37.79395|3.889|  172.0|            null|          0|\n",
      "|1506|01/03/2016 00:31:56|         01|-122.41714|37.79381|3.889|   85.0|            null|          0|\n",
      "|1506|01/03/2016 00:32:14|         01| -122.4163|37.79393|3.889|   80.0|            null|          0|\n",
      "|1506|01/03/2016 00:32:33|         01|-122.41542|37.79401|3.889|   87.0|            null|          0|\n",
      "|1506|01/03/2016 00:33:18|         01|-122.41282|37.79435|3.889|   81.0|            null|          0|\n",
      "+----+-------------------+-----------+----------+--------+-----+-------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            SPEED|\n",
      "+-------+-----------------+\n",
      "|  count|           494778|\n",
      "|   mean|4.265489779254438|\n",
      "| stddev|4.961628821231181|\n",
      "|    min|              0.0|\n",
      "|    max|           67.778|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('SPEED').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can throw in distance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| distance|\n",
      "+---------+\n",
      "|3840.6492|\n",
      "|3842.3093|\n",
      "|3844.0718|\n",
      "| 3844.791|\n",
      "| 3846.151|\n",
      "| 3832.928|\n",
      "|3840.9644|\n",
      "|3836.4036|\n",
      "|3804.4297|\n",
      "|3716.1511|\n",
      "|3689.3696|\n",
      "| 3607.872|\n",
      "| 3553.213|\n",
      "|3524.5383|\n",
      "|3494.3171|\n",
      "|3486.0679|\n",
      "|3555.6938|\n",
      "| 3630.813|\n",
      "| 3708.431|\n",
      "|3940.2405|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from geopy import distance\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "def distance_function(lat,lon):\n",
    "    \n",
    "    start_latlon = (37.786908, -122.45656)\n",
    "    row_latlon = (float(lat), float(lon))\n",
    "    \n",
    "    return distance.distance(start_latlon, row_latlon).m\n",
    "\n",
    "\n",
    "distance_udf = udf(lambda lat, lon: distance_function(lat, lon), FloatType())\n",
    "\n",
    "df_out = df.withColumn(\"distance\", distance_udf(df['LATITUDE'], df['LONGITUDE']))\n",
    "\n",
    "# df_out.select('Date','High', 'Low', 'Open', 'Close', 'special').show()\n",
    "df_out.select('distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
